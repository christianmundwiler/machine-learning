{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c4cceb4",
   "metadata": {},
   "source": [
    "Christian Mundwiler\n",
    "CSCI 4930\n",
    "Assignment 1\n",
    "09.20.2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a08475",
   "metadata": {},
   "source": [
    "## Programming Assignment 1\n",
    "#### Machine Learning (CSCI-4930/5930)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2aca17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "965be241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the dataset into pandas dataframe\n",
    "training = pd.read_csv('dataset/training.csv',header=None,delimiter=',')\n",
    "validation = pd.read_csv('dataset/validation.csv',header=None,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5408ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-315</td>\n",
       "      <td>75</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.146667</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-243</td>\n",
       "      <td>75</td>\n",
       "      <td>0.213333</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-289</td>\n",
       "      <td>77</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.220779</td>\n",
       "      <td>0.220779</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-776</td>\n",
       "      <td>118</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.203390</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-388</td>\n",
       "      <td>77</td>\n",
       "      <td>0.226190</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.207792</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1         2         3         4         5         6         7  8\n",
       "0 -315   75  0.205128  0.230769  0.307692  0.146667  0.280000  0.240000  0\n",
       "1 -243   75  0.213333  0.186667  0.293333  0.400000  0.253333  0.186667  0\n",
       "2 -289   77  0.192308  0.230769  0.282051  0.220779  0.220779  0.272727  0\n",
       "3 -776  118  0.271186  0.203390  0.271186  0.200000  0.200000  0.291667  1\n",
       "4 -388   77  0.226190  0.238095  0.250000  0.272727  0.272727  0.207792  1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d286b5",
   "metadata": {},
   "source": [
    "## Task : A\n",
    "Print total number of samples in the validation dataset: \"dataset/validation.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f3220d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the validation dataset: 60000\n"
     ]
    }
   ],
   "source": [
    "# print number of samples\n",
    "print(\"Number of samples in the validation dataset:\", len(validation.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a71fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "238ab297",
   "metadata": {},
   "source": [
    "## Task : B\n",
    "Print two numbers in the format: [n0, n1], where\n",
    "n0 represents number of class=0 (negative) samples and n1 represents number of class=1 (positive) samples in the validation dataset: \"dataset/validation.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6dabf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and store negative, postive samples in dataset\n",
    "neg = validation.loc[validation[8] == 0]\n",
    "pos = validation.loc[validation[8] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c860624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find counts for each\n",
    "n0 = len(neg.index)\n",
    "n1 = len(pos.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a450d5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40060, 19940]\n"
     ]
    }
   ],
   "source": [
    "# create and print list\n",
    "list = [n0, n1]\n",
    "print(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e9d539",
   "metadata": {},
   "source": [
    "## Task : C\n",
    "Print standard deviation of the second feature: \"The length of shorter sequence\" of the validation dataset: \"dataset/validation.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aea36c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of the length of the shorter sequence: 21.278652371559346\n"
     ]
    }
   ],
   "source": [
    "# find and print standard deviation of second feature using .std() function\n",
    "std_dev_second = validation[1].std()\n",
    "print(\"Standard deviation of the length of the shorter sequence:\", std_dev_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0876ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25100733",
   "metadata": {},
   "source": [
    "## Task : D\n",
    "Print median (i.e., 50% percentile) of the seventh feature: \"'U' frequencies of sequence 2\" of the validation dataset: \"dataset/validation.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f148e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median of 'U' frequencies of sequence 2: 0.2203389999999999\n"
     ]
    }
   ],
   "source": [
    "median_seventh = validation[6].median()\n",
    "print(\"Median of 'U' frequencies of sequence 2:\", median_seventh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184f330d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3ca953f",
   "metadata": {},
   "source": [
    "## Task : E\n",
    "Complete the function \"confusion_matrix\" partially defined that takes two arrays of target variable \"y\": y_actual and y_pred denoting ground truth class labels and predicted class labels for the N samples when N is the length of both the arrays. The function should return a list of 4 metrics: TN, FP, FN, TP (in this order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe603a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_actual, y_pred):\n",
    "    # the function takes two arrays of target variable \"y\": y_actual and y_pred\n",
    "    #    denoting ground truth class labels and predicted class labels for the N samples\n",
    "    #    when N is the length of both the arrays.\n",
    "    # The function should return a list of 4 metrics: TN, FP, FN, TP (in this order).\n",
    "    assert(len(y_actual)==len(y_pred))\n",
    "    \n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    # iterate through predicted and actual values, comparing at\n",
    "    # each step, incrementing metrics accordingly\n",
    "    for actual, predicted in zip(y_actual, y_pred):\n",
    "        if actual == 0:\n",
    "            if predicted == 0:\n",
    "                TN += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "\n",
    "        if actual == 1:\n",
    "            if predicted == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "\n",
    "    return [TN, FP, FN, TP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68881de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix([1, 0, 1, 1], [0, 0, 1, 1])) #Expected to print: [1, 0, 1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c4a133",
   "metadata": {},
   "source": [
    "## Task : F\n",
    "You need to complete the accuracy function partially defined in the file that takes a confusion matrix, i.e. the list of the four metrics: [TN, FP, FN, TP], in this order, and return accuracy. In case of Division by Zero error, return -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1524c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return accuracy\n",
    "    [TN, FP, FN, TP] = conf_mat\n",
    "    \n",
    "    acc_value = 0\n",
    "    \n",
    "    if (TN + FP + FN + TP != 0):\n",
    "        # divide accurate predictions by all predictions \n",
    "        # to determine accuracy of predictions\n",
    "        acc_value = (TN + TP) / (TN + FP + FN + TP)\n",
    "    # account for division by zero\n",
    "    else:\n",
    "        acc_value = -1\n",
    "    \n",
    "    return acc_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "610b9b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix([1, 0, 1, 1], [0, 0, 1, 1])  \n",
    "#Expected to print 0.75\n",
    "print(accuracy(conf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aa78b6",
   "metadata": {},
   "source": [
    "## Task : G\n",
    "You need to complete the precision function partially defined in the file that takes a confusion matrix, i.e. the list of the four metrics: [TN, FP, FN, TP], in this order, and return precision. It is also known as Positive Predictive Value (PPV). In case of Division by Zero error, return -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6eb55f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return precision. It is also known as Positive Predictive Value (PPV)\n",
    "    [TN, FP, FN, TP] = conf_mat\n",
    "    prec_value = 0\n",
    "\n",
    "    if (FP + TP != 0):\n",
    "        # divide true positive predictions by sum of false and true positive\n",
    "        # predictions to determine precision of positive predictions\n",
    "        prec_value = TP / (FP + TP)\n",
    "    # account for division by zero\n",
    "    else:\n",
    "        prec_value = -1\n",
    "\n",
    "    return prec_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34c02590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix([1, 0, 1, 1], [0, 0, 1, 1])  \n",
    "#Expected to print 1.0\n",
    "print(precision(conf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90ba6fe",
   "metadata": {},
   "source": [
    "## Task : H\n",
    "You need to complete the recall function partially defined in the file that takes a confusion matrix, i.e. the list of the four metrics: [TN, FP, FN, TP], in this order, and return recall. It is also known as Sensitivity, or True Positive Rate (TPR). In case of Division by Zero error, return -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "501cd309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return recall. It is also known as Sensitivity, or True Positive Rate (TPR)\n",
    "    [TN, FP, FN, TP] = conf_mat\n",
    "    rec_value = 0\n",
    "    \n",
    "    if (TP + FN) != 0:\n",
    "        # divide true predictions by sum of true positive and \n",
    "        # false negative predicions to determine TPR\n",
    "        rec_value = TP / (TP + FN)\n",
    "    # account for division by zero\n",
    "    else:\n",
    "        rec_value = -1\n",
    "    \n",
    "    return rec_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b29ef9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix([1, 0, 1, 1], [0, 0, 1, 1])  #Expected to print 0.6666666666\n",
    "print(recall(conf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f6c9a3",
   "metadata": {},
   "source": [
    "## Task : I\n",
    "You need to complete the F1 function partially defined in the file that takes a confusion matrix, i.e. the list of the four metrics: [TN, FP, FN, TP], in this order, and return F1. It is the harmonic mean of precision and recall. In case of Division by Zero error, return -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cab4cd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from curses import tparm\n",
    "\n",
    "\n",
    "def F1(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return F1 score. It is the harmonic mean of precision and recall\n",
    "    [TN, FP, FN, TP] = conf_mat\n",
    "    f1_value = 0\n",
    "    \n",
    "    # use previously defined functions to find precision (PPV) and recall (TPR)\n",
    "    ppv = precision(conf_mat)\n",
    "    tpr = recall(conf_mat)\n",
    "\n",
    "    if (ppv + tpr != 0):\n",
    "        # use f1 formula to find value\n",
    "        f1_value = 2 * ((ppv * tpr) / (ppv + tpr))\n",
    "    # account for division by zero\n",
    "    else:\n",
    "        f1_value = -1\n",
    "    \n",
    "    return f1_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcc93aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix([1, 0, 1, 1], [0, 0, 1, 1]) #Expected to print 0.8\n",
    "print(F1(conf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2128d772",
   "metadata": {},
   "source": [
    "## Task : J\n",
    "You need to complete the MCC function defined in the file that takes a confusion matrix, i.e. the list of the four metrics: [TN, FP, FN, TP], in this order, and return Matthews Correlation Coefficient (MCC). In case of Division by Zero error, return -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01ca3913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def MCC(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return Matthews correlation coefficient (MCC)\n",
    "    [TN, FP, FN, TP] = conf_mat\n",
    "    mcc_value = 0\n",
    "    \n",
    "    # find numerator and denominator for MCC function\n",
    "    num = ((TP*TN) - (FP*FN))\n",
    "    denom = (TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)\n",
    "\n",
    "    if (denom > 0):\n",
    "        # calculate MCC value using its function\n",
    "        mcc_value = num / sqrt(denom)\n",
    "    # account for division by zero\n",
    "    else:\n",
    "        mcc_value = -1\n",
    "    \n",
    "    return mcc_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b8284c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5773502691896258\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix([1, 0, 1, 1], [0, 0, 1, 1]) #Expected to print 0.5773502691896258\n",
    "print(MCC(conf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be81ef54",
   "metadata": {},
   "source": [
    "## Task : K\n",
    "You need to complete the FDR function defined in the file that takes a confusion matrix, i.e. the list of the four metrics: [TN, FP, FN, TP], in this order, and return False Discovery Rate (FDR). In case of Division by Zero error, return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15bf9850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FDR(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return False Discovery Rate (FDR)cd ~~~~``\n",
    "    [TN, FP, FN, TP] = conf_mat\n",
    "    fdr_value = 0\n",
    "    \n",
    "    # FDR = 1 - PPV\n",
    "\n",
    "    # find PPV\n",
    "    ppv = precision(conf_mat)\n",
    "    \n",
    "    # find FDR\n",
    "    if (ppv != -1):\n",
    "        fdr_value = 1 - ppv\n",
    "    else:\n",
    "        fdr_value = -1\n",
    "    \n",
    "    return fdr_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dffbe6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix([1, 0, 1, 1], [0, 0, 1, 1]) #Expected to print 0.0\n",
    "print(FDR(conf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9614d2f8",
   "metadata": {},
   "source": [
    "## Task : L\n",
    "Print as a dataframe containing:\n",
    " {model_name,acc,prec,rec,f1,mcc,FDR} for each of the N models (listed in models/*) after predicting the target variables of the validation data: \"dataset/validation.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85e17a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:50<00:00, 18.48s/it]\n"
     ]
    }
   ],
   "source": [
    "#Print as a dataframe containing:\n",
    "# {model_name,acc,prec,rec,f1,mcc,FDR} for each of the N models (listed in model_files) predicting the target variables\n",
    "#  of the validation data.\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "result = pd.DataFrame(columns=['model_name','Accuracy','Precision','Recall','F1','MCC','FDR'])\n",
    "\n",
    "model_files=[\"models/Model_1.pkl\",\"models/Model_2.pkl\",\"models/Model_3.pkl\", \"models/Model_4.pkl\",\"models/Model_5.pkl\",\"models/Model_6.pkl\"]\n",
    "\n",
    "for file_name in tqdm(model_files):\n",
    "    in_file = open(file_name,'rb')\n",
    "    model = pickle.load(in_file)\n",
    "    in_file.close()\n",
    "\n",
    "    #Do the prediction by calling the \"predict\" member function of the model object on the\n",
    "    # validation data with the 8 features\n",
    "    X_test = validation.iloc[:,:-1]\n",
    "    y_test = validation.iloc[:,-1]\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # use previously defined functions to compute values\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    acc = accuracy(conf_mat)\n",
    "    prec = precision(conf_mat)\n",
    "    rec = recall(conf_mat)\n",
    "    f1 = F1(conf_mat)\n",
    "    mcc = MCC(conf_mat)\n",
    "    fdr = FDR(conf_mat)\n",
    "    \n",
    "    result = result.append(pd.Series([file_name,acc,prec,rec,f1,mcc,fdr],index=result.columns),ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eab79ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           model_name  Accuracy  Precision   Recall        F1       MCC  \\\n",
      "0  models/Model_1.pkl  0.337583   0.334082  0.99995  0.500835  0.050929   \n",
      "1  models/Model_2.pkl  0.334083   0.332910  0.99995  0.499518  0.028982   \n",
      "2  models/Model_3.pkl  0.667667  -1.000000  0.00000  0.000000 -1.000000   \n",
      "3  models/Model_4.pkl  0.390483   0.352497  0.99659  0.520789  0.168805   \n",
      "4  models/Model_5.pkl  0.333583   0.332744  0.99995  0.499330  0.024302   \n",
      "5  models/Model_6.pkl  0.337500   0.334054  0.99995  0.500804  0.050516   \n",
      "\n",
      "        FDR  \n",
      "0  0.665918  \n",
      "1  0.667090  \n",
      "2 -1.000000  \n",
      "3  0.647503  \n",
      "4  0.667256  \n",
      "5  0.665946  \n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1388167d",
   "metadata": {},
   "source": [
    "## Task : M\n",
    "Print the model name with path which is performing superior among the 6 pretrained models in terms of accuracy, given the performance result dataframe from “L”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1552ca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model with the highest accuracy value: models/Model_3.pkl\n"
     ]
    }
   ],
   "source": [
    "# find index of model with highest accuracy value\n",
    "highest_acc_index = result['Accuracy'].idxmax()\n",
    "\n",
    "# find model name and print\n",
    "model_acc = result['model_name'][highest_acc_index]\n",
    "print(\"The model with the highest accuracy value:\", model_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa229f32",
   "metadata": {},
   "source": [
    "## Task : N\n",
    "Print the model name with path which is performing the worst among the 6 pretrained models in terms of recall, given the performance result dataframe from “L”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca013dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e940ab1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model with the worst recall value: models/Model_3.pkl\n"
     ]
    }
   ],
   "source": [
    "# find index of model with worst recall value\n",
    "worst_recall_index = result['Recall'].idxmin()\n",
    "\n",
    "# find model name and print\n",
    "model_rec = result['model_name'][worst_recall_index]\n",
    "print(\"The model with the worst recall value:\", model_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a6aac4",
   "metadata": {},
   "source": [
    "## Task : O\n",
    "Scale all the features of the validation set using the formula, z = (x-m)/s,\n",
    "    \n",
    "    where m = mean of a feature in the training set: \"dataset/training.csv\"\n",
    "    \n",
    "    s = standard deviation of the feature in the training set: \"dataset/training.csv\"\n",
    "    \n",
    "    #  DO NOT SCALE the target feature.\n",
    "    \n",
    "    # At the end, return a tuple (X, y), with X being a numpy array of shape (N,8) and y is an N dim array and \n",
    "\n",
    "N is the total number of samples in the validation set: \"dataset/validation.csv\".\n",
    "\n",
    "Store the scaled dataset in a variable (preferably a dataframe) named “X_validation_scaled”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0aa491fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.775328</td>\n",
       "      <td>-1.002997</td>\n",
       "      <td>-0.325953</td>\n",
       "      <td>0.167255</td>\n",
       "      <td>1.069653</td>\n",
       "      <td>-1.676320</td>\n",
       "      <td>1.387452</td>\n",
       "      <td>-0.658364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.155149</td>\n",
       "      <td>-1.002997</td>\n",
       "      <td>-0.163911</td>\n",
       "      <td>-0.761209</td>\n",
       "      <td>0.723327</td>\n",
       "      <td>3.906838</td>\n",
       "      <td>0.758204</td>\n",
       "      <td>-2.059975</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.912485</td>\n",
       "      <td>-0.910311</td>\n",
       "      <td>-0.579139</td>\n",
       "      <td>0.167255</td>\n",
       "      <td>0.451215</td>\n",
       "      <td>-0.042980</td>\n",
       "      <td>-0.009957</td>\n",
       "      <td>0.201713</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.656587</td>\n",
       "      <td>0.989744</td>\n",
       "      <td>0.978644</td>\n",
       "      <td>-0.409145</td>\n",
       "      <td>0.189161</td>\n",
       "      <td>-0.500924</td>\n",
       "      <td>-0.500269</td>\n",
       "      <td>0.699464</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.390230</td>\n",
       "      <td>-0.910311</td>\n",
       "      <td>0.090006</td>\n",
       "      <td>0.321487</td>\n",
       "      <td>-0.321826</td>\n",
       "      <td>1.101893</td>\n",
       "      <td>1.215834</td>\n",
       "      <td>-1.504802</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.775328 -1.002997 -0.325953  0.167255  1.069653 -1.676320  1.387452   \n",
       "1  1.155149 -1.002997 -0.163911 -0.761209  0.723327  3.906838  0.758204   \n",
       "2  0.912485 -0.910311 -0.579139  0.167255  0.451215 -0.042980 -0.009957   \n",
       "3 -1.656587  0.989744  0.978644 -0.409145  0.189161 -0.500924 -0.500269   \n",
       "4  0.390230 -0.910311  0.090006  0.321487 -0.321826  1.101893  1.215834   \n",
       "\n",
       "          7  8  \n",
       "0 -0.658364  0  \n",
       "1 -2.059975  0  \n",
       "2  0.201713  0  \n",
       "3  0.699464  1  \n",
       "4 -1.504802  1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import X_OK\n",
    "from re import I, S\n",
    "\n",
    "def scaleFeatures():\n",
    "    # find mean of each feature excapt target and store in a list\n",
    "    mean = []\n",
    "    for col in range(8):\n",
    "        mean.append(training[col].mean())\n",
    "\n",
    "    # find standard deviation of each feature except target and \n",
    "    # store in a list\n",
    "    std = []\n",
    "    for col in range(8):\n",
    "        std.append(training[col].std())\n",
    "    \n",
    "    # populate y with target feature\n",
    "    y = np.array(validation[8])\n",
    "\n",
    "    # create numpy array of shape (N, 8) where N is total number\n",
    "    # of samples in validation dataset, and fill array with zeroes\n",
    "    num_samples = len(validation.index)\n",
    "    X = np.zeros((num_samples, 8))\n",
    "    \n",
    "    # populate X with scaled values using given formula\n",
    "    # ignore target feature in validation\n",
    "    for col in range(8):\n",
    "        X[:, col] = (validation[col] - mean[col]) / std[col]\n",
    "\n",
    "    # store in tupel and return\n",
    "    scaled_tuple = (X, y)\n",
    "    return scaled_tuple\n",
    "\n",
    "# scale dataset and store in dataframe\n",
    "scaled_df = scaleFeatures()\n",
    "X_validation_scaled = pd.DataFrame(scaled_df[0], columns=['0', '1', '2', '3', '4', '5', '6', '7'])\n",
    "X_validation_scaled[8] = scaled_df[1]\n",
    "X_validation_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d7b7c",
   "metadata": {},
   "source": [
    "## Task : P\n",
    "Print as a dataframe containing:\n",
    "  {model_name,acc,prec,rec,f1,mcc,FDR} for each of the N models (listed in model_files) after  predicting the target variables \"y\" (given) for \"X\" (the scaled validation dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f08e70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [02:06<00:00, 21.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# create dataframe\n",
    "scaled_df = pd.DataFrame(columns=['model_name', 'acc', 'prec', 'rec', 'f1', 'mcc', 'FDR'])\n",
    "\n",
    "# read in files\n",
    "for file_name in tqdm(model_files):\n",
    "    in_file = open(file_name,'rb')\n",
    "    model = pickle.load(in_file)\n",
    "    in_file.close()\n",
    "\n",
    "    # run scaleFeatures() function from previous step \n",
    "    scaled = scaleFeatures()\n",
    "    y_actual = scaled[1]\n",
    "    y_predicted = model.predict(scaled[0])\n",
    "\n",
    "    # create confusion matrix\n",
    "    conf_mat = confusion_matrix(y_actual, y_predicted)\n",
    "\n",
    "    # find metrics associated with confusion matrix\n",
    "    acc = accuracy(conf_mat)\n",
    "    prec = precision(conf_mat)\n",
    "    rec = recall(conf_mat)\n",
    "    f1 = F1(conf_mat)\n",
    "    mcc = MCC(conf_mat)\n",
    "    fdr = FDR(conf_mat)\n",
    "\n",
    "    # add data to dataframe\n",
    "    scaled_df.loc[len(scaled_df.index)] = [file_name, acc, prec, rec, f1, mcc, fdr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4040548b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           model_name       acc      prec       rec        f1       mcc  \\\n",
      "0  models/Model_1.pkl  0.951233  0.944276  0.906770  0.925143  0.889404   \n",
      "1  models/Model_2.pkl  0.947700  0.936597  0.903811  0.919912  0.881411   \n",
      "2  models/Model_3.pkl  0.965283  0.946492  0.949198  0.947843  0.921828   \n",
      "3  models/Model_4.pkl  0.966100  0.948907  0.949097  0.949002  0.923614   \n",
      "4  models/Model_5.pkl  0.914133  0.887770  0.848947  0.867925  0.804802   \n",
      "5  models/Model_6.pkl  0.953883  0.936816  0.923521  0.930121  0.895760   \n",
      "\n",
      "        FDR  \n",
      "0  0.055724  \n",
      "1  0.063403  \n",
      "2  0.053508  \n",
      "3  0.051093  \n",
      "4  0.112230  \n",
      "5  0.063184  \n"
     ]
    }
   ],
   "source": [
    "print(scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac093d5",
   "metadata": {},
   "source": [
    "## Task : Q\n",
    "Print the model name with path which is performing superior in terms of accuracy, given the performance result dataframe from “P”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be1a6362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model with the highest accuracy value: models/Model_4.pkl\n"
     ]
    }
   ],
   "source": [
    "# find index of model with highest accuracy value\n",
    "highest_acc_index_scaled = scaled_df['acc'].idxmax()\n",
    "\n",
    "# find model name and print\n",
    "model_acc = scaled_df['model_name'][highest_acc_index_scaled]\n",
    "print(\"The model with the highest accuracy value:\", model_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5249c432",
   "metadata": {},
   "source": [
    "## Task : R\n",
    "Print the model name with path which is performing the worst in terms of recall, given the performance result dataframe from “P”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95773261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model with the lowest recall value: models/Model_5.pkl\n"
     ]
    }
   ],
   "source": [
    "# find index of model with lowest result value\n",
    "lowest_rec_index = scaled_df['rec'].idxmin()\n",
    "\n",
    "# find model name and print\n",
    "model_rec = scaled_df['model_name'][lowest_rec_index]\n",
    "print(\"The model with the lowest recall value:\", model_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6efd5f",
   "metadata": {},
   "source": [
    "## Task : S\n",
    "Flip the prediction of Model 1, and then compute and\n",
    " print as a dataframe containing: {acc,prec,rec,f1,mcc,FDR} \n",
    "on the original (i.e., not-scaled) validation dataset: \"dataset/validation.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c776b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 56.91it/s]\n"
     ]
    }
   ],
   "source": [
    "model_file=[\"models/Model_1.pkl\"]\n",
    "\n",
    "for file_name in tqdm(model_file):\n",
    "    in_file = open(file_name,'rb')\n",
    "    model = pickle.load(in_file)\n",
    "    in_file.close()\n",
    "\n",
    "    #\n",
    "    X_test = validation.iloc[:,:-1]\n",
    "    y_test = validation.iloc[:,-1]\n",
    "    y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b7e6b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip predictions\n",
    "y_pred_flipped = y_pred^1\n",
    "\n",
    "# use previously defined functions to compute values\n",
    "conf_mat = confusion_matrix(y_test, y_pred_flipped)\n",
    "acc = accuracy(conf_mat)\n",
    "prec = precision(conf_mat)\n",
    "rec = recall(conf_mat)\n",
    "f1 = F1(conf_mat)\n",
    "mcc = MCC(conf_mat)\n",
    "fdr = FDR(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "129b5a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        acc      prec      rec        f1       mcc       FDR\n",
      "0  0.662417  0.003155  0.00005  0.000099 -0.050929  0.996845\n"
     ]
    }
   ],
   "source": [
    "# create add data to dataframe\n",
    "flipped_prediction = pd.DataFrame(columns=['acc','prec','rec','f1','mcc','FDR'])\n",
    "flipped_prediction.loc[len(flipped_prediction.index)] = [acc, prec, rec, f1, mcc, fdr]\n",
    "\n",
    "print(flipped_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7901669d",
   "metadata": {},
   "source": [
    "## Task : T\n",
    "Say, in a confusion matrix, the values of the four metrics are: TP=90, TN=1, FP=4, FN=5. Compute F1_original and MCC_original denoting the F1 and MCC scores.\n",
    "\n",
    "Now, flip the predictions, i.e., positives are now will be predicted as negative, and negatives are going to be predicted as positive. \n",
    "\n",
    "Then, compute F1_flipped and MCC_flipped, denoting corresponding F1 and MCC scores. \n",
    "\n",
    "Print/Return the new {TP, TN, FP, FN, F1_original,MCC_original,F1_flipped, MCC_flipped, COMMENT_string} as a dataframe, where COMMENT_string is a string that will be no longer than 200 characters but is going to be your comment about the F1 and MCC values for the two cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bcd5c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)  # or 199\n",
    "\n",
    "#[TN, FP, FN, TP] is correct order:\n",
    "confusion = [1, 4, 5, 90]\n",
    "\n",
    "F1_original = F1(confusion)\n",
    "MCC_original = MCC(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93157967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1, 90, 5]\n"
     ]
    }
   ],
   "source": [
    "# When flipping predictions, TP <-> FN & TN <-> FP\n",
    "# Therefore, we can reorder the confusion matrix:\n",
    "def flip_conf_matr(matrix):\n",
    "\n",
    "    matrix_reorder = matrix.copy()\n",
    "    matrix_reorder[0] = matrix[1]\n",
    "    matrix_reorder[1] = matrix[0]\n",
    "    matrix_reorder[2] = matrix[3]\n",
    "    matrix_reorder[3] = matrix[2]\n",
    "\n",
    "    return matrix_reorder\n",
    "\n",
    "# reorder matrix with flipped predictions\n",
    "flipped_confusion = flip_conf_matr(confusion)\n",
    "print(flipped_confusion)\n",
    "\n",
    "# run F1, MCC functions on flipped conf matrix\n",
    "F1_flipped = F1(flipped_confusion)\n",
    "MCC_flipped = MCC(flipped_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62d084c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>F1_original</th>\n",
       "      <th>MCC_original</th>\n",
       "      <th>F1_flipped</th>\n",
       "      <th>MCC_flipped</th>\n",
       "      <th>COMMENT_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>5</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.135242</td>\n",
       "      <td>0.09901</td>\n",
       "      <td>-0.135242</td>\n",
       "      <td>Original F1 value is much higher than flipped, indicating original prediction was better that its opposite, while the MCC value is just negative of the original.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TP TN  FP FN  F1_original  MCC_original  F1_flipped  MCC_flipped  \\\n",
       "0  4  1  90  5     0.952381      0.135242     0.09901    -0.135242   \n",
       "\n",
       "                                                                                                                                                      COMMENT_string  \n",
       "0  Original F1 value is much higher than flipped, indicating original prediction was better that its opposite, while the MCC value is just negative of the original.  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe\n",
    "flipped_conf_df = pd.DataFrame(columns=['TP','TN','FP','FN', 'F1_original', 'MCC_original', 'F1_flipped', 'MCC_flipped', 'COMMENT_string'])\n",
    "\n",
    "# personal comment on effect of flipping prediction on F1, MCC values\n",
    "comment = \"Original F1 value is much higher than flipped, indicating original prediction was better that its opposite, while the MCC value is just negative of the original.\"\n",
    "\n",
    "# add to dataframe\n",
    "flipped_conf_df.loc[len(flipped_conf_df.index)] = [flipped_confusion[0], flipped_confusion[1], flipped_confusion[2], flipped_confusion[3], F1_original, MCC_original, F1_flipped, MCC_flipped, comment]\n",
    "flipped_conf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ba9996",
   "metadata": {},
   "source": [
    "## For Graduate Students only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a72e6c",
   "metadata": {},
   "source": [
    "## Task: U\n",
    "This task is a follow up of Task P. There is always cost associated with misclassifications. For instance, if a model predicts a ncRNA (class=1) to be non ncRNA (class=0), further verfication will then follow that includes going through the next generation sequencing of those samples costing USD 20 per sample. On the other hand, cost of predicting a non ncRNA to be ncRNA insignificant as most researchers do not care for ncRNAs. They might put it in a piece of paper as a note costing USD 1 per 5 samples. The same cost applies to correct predictions too.\n",
    "\n",
    "What is the cost of predictions with each of the six models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4027ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0a14fbd",
   "metadata": {},
   "source": [
    "## Task: V\n",
    "Please comment on which of the six models is the best on the cost basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa50cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccf512d2",
   "metadata": {},
   "source": [
    "## Task: W\n",
    "Please comment on which of the six models is the best overall. Explain your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15409d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e9cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be2434a5",
   "metadata": {},
   "source": [
    "## Task: X\n",
    "Again a followup of Tasks O and P: Please scale the given validation dataset with an alternate scaling technique you can think of and repeat Task P with the modified scaled validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ff9a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3adf9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdf41c13",
   "metadata": {},
   "source": [
    "## Task: Y\n",
    "Print the model name with path which is performing superior in terms of accuracy, given the performance result dataframe from “X”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e3428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7f35599",
   "metadata": {},
   "source": [
    "## Task: Z\n",
    "Print the model name with path which is performing the worst in terms of recall, given the performance result dataframe from “X”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea65917d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b084cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.14 ('venv-ml-pa1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "7c520b77326e501122635be36f5e26b8cd1eb7fe8dd13438e51354a8c1b0a2a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
